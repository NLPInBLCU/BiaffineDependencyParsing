# yaml 格式说明：
# （1）注意空格！yaml格式非常严格。双空格开头，冒号后必须有一个空格
# （2）字符串用单引号引起来
# （3）浮点数或者科学计数法必须用小数点（否则可能被当做字符串）
# （4）布尔类型：true，false
run:
  cuda: true
  cpu: false
  seed: 1234
  #只需要指定最大的epoch数量，不需要指定最大steps
  #可能的选择：80,150
  max_train_epochs: 80
  eval_interval: 100
  early_stop: false
  early_stop_epochs: 6
data_set:
  data_dir: 'dataset'
  train_file: 'train/text_news.train.conllu'
  dev_file: 'dev/sdp_text_dev.conllu'
  # GPU 32GB:80; 24GB:64; 16GB:40 12GB:30 10GB:20
  per_gpu_train_batch_size: 40
  # GPU <=12GB:10; >12GB:20或者30
  per_gpu_eval_batch_size: 20
  skip_too_long_input: true
  loader_worker_num: 10
  use_cache: true
merge_train_data:
  # 幂指数加权采样：在多个不同领域数据之间如何合理地采样
  # 是够采用幂指数平滑采样: utils.input_utils.custom_dataset.ConcatTensorRandomDataset
  merge_training: true
  train_text_file: 'train/sdp_text_train.conllu'
  train_news_file: 'train/sdp_news_train.conllu'
  merge_train_mode: 'exp'  # exp or prob
  # 仅在merge_train_mode == 'exp' 时生效
  merge_train_exp: 0.7
  # 仅在merge_train_mode == 'prob' 时生效
  merge_train_prob: [0.6,0.4]
output:
  output_dir: 'output'
  log_name: 'bert_parser'
  save_best_model: false
graph_vocab:
  #依存弧的vocab，必须提前生成
  graph_vocab_file: 'dataset/graph_vocab.txt'
Model:
  saved_model_path: '/search/hadoop02/suanfa/lihuayong/data/pretrain/ernie_1/'
#  saved_model_path: ''
#  saved_model_path: ''
encoder:
  #encoder的类型：bertology,transformer,lstm ....
  encoder_type: 'bertology'
  encoder_output_dim: 768
CharRNN:
Transformer:
BERTology:
  #最大长度 必须超过数据集的最大长度（字数）,新闻领域的最大句长可达233
  bertology_type: 'bert'
  max_seq_len: 100
  #ROOT的表示形式：unused,cls,root ....
  root_representation: 'unused'
  #中文单词的提取方式：s,e,s+e,s-e
  bertology_word_select: 'e'
  #BERT输出的选择方式：last,last_four_sum,last_four_cat,all_sum,attention
  bertology_output_mode: 'last_four_sum'
  bertology_after: 'transformer'
  after_layers: 2
  after_dropout: 0.2
BERTologyInputMask:
  # 是否使用InputMask
  # 类似word dropout
  input_mask: true
  input_mask_prob: 0.1
  input_mask_granularity: 'word' # char or word
BERTologyFreeze:
  # 是否冻结BERT的参数
  # 冻结底层参数可以减小显存占用，加快训练
  freeze: true # freeze的控制开关，如果为false则无论下述参数为何都不使用freeze
  freeze_bertology_layers: 3 # -1:frezze BERT embedding 层; 0：只freeze最底层; 3：freeze 0,1,2,3层; 11: freeze Base model所有层
  freeze_epochs: 'all' # all:一直freeze; first:只在首个epoch freeze:
decoder:
  biaffine_hidden_dim: 600
  biaffine_dropout: 0.33
  # direct_biaffine： 不使用全连接，直接把encoder的表示传给双仿
  direct_biaffine: false
update:
  # 是否用可学习的loss—ratio（用来控制两种loss的组合）
  learned_loss_ratio: true
  label_loss_ratio: 0.5
  # 是够缩放loss（有用吗？）
  scale_loss: false
  loss_scaling_ratio: 2
  # 标签平滑系数（现在没有实现标签平滑 todo）
  label_smoothing: 0.03
  # =0则不使用梯度裁剪
  max_grad_norm: 5.0
  # adam-bertology (huggingface版本的adamw); adamw-torch (torch 1.2); adam;
  optimizer: 'adamw-bertology'
  beta1: 0.9
  beta2: 0.99
  eps: 1.0e-12
  weight_decay: 3.0e-9
  learning_rate: 5.0e-5  # 学习率
  different_lr: true  # 如果为True，则使用下面的两个学习率
  bertology_lr: 5.0e-5
  other_lr: 1.0e-3
  # bertDistill:1.0e-6;
  adam_epsilon: 1.0e-8
  # bertDistill:0.05;
  warmup_prop: 0.1
  # 是够按照单词数量平均一个batch的loss
  average_loss_by_words_num: true
